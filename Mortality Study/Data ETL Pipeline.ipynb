{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output for the event count metrics is :\n",
      "(1, 8635, 982.014, 1, 12627, 498.118)\n",
      "\n",
      "output for the encounter count metrics is :\n",
      "(1, 203, 23.038, 1, 391, 15.452)\n",
      "\n",
      "output for the record length metrics is :\n",
      "(0, 1972, 127.532, 0, 2914, 159.2)\n"
     ]
    }
   ],
   "source": [
    "def read_csv(filepath):\n",
    "    '''\n",
    "    Read the events.csv and mortality_events.csv files. \n",
    "    Variables returned from this function are passed as input to the metric functions.\n",
    "    '''\n",
    "    events = pd.read_csv(filepath + 'events.csv')\n",
    "    mortality = pd.read_csv(filepath + 'mortality_events.csv')\n",
    "    return events, mortality\n",
    "\n",
    "def event_count_metrics(events, mortality):\n",
    "    '''\n",
    "    Implement this function to return the event count metrics.\n",
    "    Event count is defined as the number of events recorded for a given patient.\n",
    "    '''\n",
    "    events_2 = events.copy()\n",
    "    events_2 = events_2.assign(deceased=events_2['patient_id'].isin(mortality['patient_id']).astype(int))\n",
    "    surviveData = events_2.copy()\n",
    "    surviveData = surviveData.loc[surviveData['deceased'] == 0]\n",
    "    deceasedData = events_2.copy()\n",
    "    deceasedData = deceasedData.loc[deceasedData['deceased'] == 1]\n",
    "\n",
    "    deceasedEventCount = deceasedData['event_id'].count()\n",
    "    deceasedDistincPatient = deceasedData['patient_id'].nunique()\n",
    "    deceasedData['Agg'] = deceasedData.groupby('patient_id')['patient_id'].transform('count')\n",
    "    \n",
    "    Meandeceased_EventC = deceasedEventCount/deceasedDistincPatient\n",
    "    Maxdeceased_EventC = deceasedData['Agg'].max()\n",
    "    Mindeceased_EventC = deceasedData['Agg'].min()\n",
    "\n",
    "    surviveEventCount = surviveData['event_id'].count()\n",
    "    surviveDistincPatient = surviveData['patient_id'].nunique()\n",
    "    surviveData['Agg'] = surviveData.groupby('patient_id')['patient_id'].transform('count')\n",
    "\n",
    "    Meansurvive_EventC = surviveEventCount/surviveDistincPatient\n",
    "    Maxsurvive_EventC = surviveData['Agg'].max()\n",
    "    Minsurvive_EventC = surviveData['Agg'].min()\n",
    "\n",
    "    return Mindeceased_EventC, Maxdeceased_EventC, Meandeceased_EventC, Minsurvive_EventC, Maxsurvive_EventC, Meansurvive_EventC\n",
    "\n",
    "def encounter_count_metrics(events, mortality):\n",
    "    '''\n",
    "    Implement this function to return the encounter count metrics.\n",
    "    Encounter count is defined as the count of unique dates on which a given patient visited the ICU. \n",
    "    '''\n",
    "    events_2 = events.copy()\n",
    "    events_2 = events_2.assign(deceased=events_2['patient_id'].isin(mortality['patient_id']).astype(int))\n",
    "    surviveData = events_2.copy()\n",
    "    surviveData = surviveData.loc[surviveData['deceased'] == 0]\n",
    "    surviveData = surviveData.drop(['deceased', 'event_id', 'event_description', 'value'], axis=1)\n",
    "    surviveData = surviveData.drop_duplicates()\n",
    "    deceasedData = events_2.copy()\n",
    "    deceasedData = deceasedData.loc[deceasedData['deceased'] == 1]\n",
    "    deceasedData = deceasedData.drop(['deceased', 'event_id', 'event_description', 'value'], axis=1)\n",
    "    deceasedData = deceasedData.drop_duplicates()\n",
    "    \n",
    "    deceased_timestamp = deceasedData['timestamp'].count()\n",
    "    deceased_patients = deceasedData['patient_id'].nunique()\n",
    "    deceasedData['Agg'] = deceasedData.groupby('patient_id')['patient_id'].transform('count')\n",
    "    \n",
    "    Meandeceased_EncounC = deceased_timestamp/deceased_patients\n",
    "    Maxdeceased_EncounC = deceasedData['Agg'].max()\n",
    "    Mindeceased_EncounC = deceasedData['Agg'].min()\n",
    "\n",
    "    survive_timestamp = surviveData['timestamp'].count()\n",
    "    survive_patients = surviveData['patient_id'].nunique()\n",
    "    surviveData['Agg'] = surviveData.groupby('patient_id')['patient_id'].transform('count')\n",
    "    \n",
    "    Meansurvive_EncounC = survive_timestamp/survive_patients\n",
    "    Maxsurvive_EncounC = surviveData['Agg'].max()\n",
    "    Minsurvive_EncounC = surviveData['Agg'].min()\n",
    "\n",
    "    return Mindeceased_EncounC, Maxdeceased_EncounC, Meandeceased_EncounC, Minsurvive_EncounC, Maxsurvive_EncounC, Meansurvive_EncounC\n",
    "\n",
    "def record_length_metrics(events, mortality):\n",
    "    '''\n",
    "    Implement this function to return the record length metrics.\n",
    "    Record length is the duration between the first event and the last event for a given patient. \n",
    "    '''\n",
    "    events_2 = events.copy()\n",
    "    events_2['timestamp'] = pd.to_datetime(events_2['timestamp'])    \n",
    "    LengthMetrics = events_2[['patient_id','timestamp']].groupby('patient_id').agg([max,min])\n",
    "    LengthMetrics['duration'] = LengthMetrics['timestamp']['max']-LengthMetrics['timestamp']['min']\n",
    "    LengthMetrics['days']=LengthMetrics['duration'].dt.days\n",
    "    LengthMetrics=LengthMetrics.reset_index()\n",
    "    LengthMetrics = LengthMetrics[['patient_id', 'days']]\n",
    "    deathData1 = LengthMetrics[LengthMetrics['patient_id'].isin(mortality['patient_id'])]    \n",
    "    deathData = deathData1['days']  \n",
    "    Meandeceased_LenMetrics = deathData.mean()\n",
    "    Maxdeceased_LenMetrics = deathData.max()\n",
    "    Mindeceased_LenMetrics = deathData.min()\n",
    "    surviveData = LengthMetrics[-LengthMetrics['patient_id'].isin(mortality['patient_id'])]\n",
    "    surviveData = surviveData['days']\n",
    "    Meansurvive_LenMetrics = surviveData.mean()\n",
    "    Maxsurvive_LenMetrics = surviveData.max()\n",
    "    Minsurvive_LenMetrics = surviveData.min()    \n",
    "    return Mindeceased_LenMetrics, Maxdeceased_LenMetrics, Meandeceased_LenMetrics, Minsurvive_LenMetrics, Maxsurvive_LenMetrics, Meansurvive_LenMetrics\n",
    "\n",
    "def main():\n",
    "    \n",
    "    train_path = 'data/'\n",
    "    events, mortality = read_csv(train_path)\n",
    "\n",
    "    #event count metrics\n",
    "    print(\"output for the event count metrics is :\") \n",
    "    print(event_count_metrics(events, mortality))\n",
    "    print()\n",
    "    #encounter count metrics    \n",
    "    print(\"output for the encounter count metrics is :\")\n",
    "    print(encounter_count_metrics(events, mortality)) \n",
    "    print()\n",
    "    #record length metrics    \n",
    "    print(\"output for the record length metrics is :\")\n",
    "    print( record_length_metrics(events, mortality))\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature construction\n",
    "import utils\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def read_csv(filepath):    \n",
    "    '''    \n",
    "    Read the events.csv, mortality_events.csv and event_feature_map.csv files into events, mortality and feature_map.\n",
    "    Return events, mortality and feature_map\n",
    "    '''\n",
    "    #Columns in events.csv - patient_id,event_id,event_description,timestamp,value\n",
    "    events = pd.read_csv(filepath + 'events.csv')\n",
    "    \n",
    "    #Columns in mortality_event.csv - patient_id,timestamp,label\n",
    "    mortality = pd.read_csv(filepath + 'mortality_events.csv')\n",
    "\n",
    "    #Columns in event_feature_map.csv - idx,event_id\n",
    "    feature_map = pd.read_csv(filepath + 'event_feature_map.csv')\n",
    "    return events, mortality, feature_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_index_date(events, mortality, deliverables_path):\n",
    "    \n",
    "    '''\n",
    "    1. Create list of patients alive ( mortality_events.csv only contains information about patients deceased)\n",
    "    2. Split events into two groups based on whether the patient is alive or deceased\n",
    "    3. Calculate index date for each patient\n",
    "    \n",
    "    IMPORTANT:\n",
    "    Save indx_date to a csv file in the deliverables folder named as etl_index_dates.csv. \n",
    "    Use the global variable deliverables_path while specifying the filepath. \n",
    "    Each row is of the form patient_id, indx_date.\n",
    "    The csv file should have a header \n",
    "    For example if you are using Pandas, you could write: \n",
    "        indx_date.to_csv(deliverables_path + 'etl_index_dates.csv', columns=['patient_id', 'indx_date'], index=False)\n",
    "\n",
    "    Return indx_date\n",
    "    '''\n",
    "    events_3 = events.copy()\n",
    "    events_3 = events_3.assign(deceased=events_3['patient_id'].isin(mortality['patient_id']).astype(int))\n",
    "    surviveData = events_3.copy()\n",
    "    surviveData = surviveData.loc[surviveData['deceased'] == 0]\n",
    "    surviveData = surviveData.drop(['deceased', 'event_id', 'event_description', 'value'], axis=1)\n",
    "    surviveData = surviveData.drop_duplicates()\n",
    "    surviveData['timestamp'] =pd.to_datetime(surviveData.timestamp)\n",
    "    surviveDataFinal = surviveData.loc[surviveData.groupby('patient_id').timestamp.idxmax()]\n",
    "    \n",
    "    deceasedData = mortality.copy()\n",
    "    deceasedData= deceasedData[['patient_id','timestamp']]\n",
    "    deceasedData['timestamp']= pd.to_datetime(deceasedData['timestamp'])\n",
    "    deceasedData['timestamp']= deceasedData['timestamp']-timedelta(days = 30)\n",
    "    \n",
    "    indx_date = pd.concat([surviveDataFinal,deceasedData]).reset_index(drop=True)\n",
    "    indx_date = indx_date.rename(columns={'timestamp': 'indx_date'})\n",
    "    indx_date.to_csv(deliverables_path + 'etl_index_dates.csv', columns=['patient_id', 'indx_date'], index=False)\n",
    "    \n",
    "    return indx_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_events(events, indx_date, deliverables_path):\n",
    "    \n",
    "    '''\n",
    "    1. Join indx_date with events on patient_id\n",
    "    2. Filter events occuring in the observation window(IndexDate-2000 to IndexDate)\n",
    "    Save filtered_events to a csv file in the deliverables folder named as etl_filtered_events.csv. \n",
    "    Use the global variable deliverables_path while specifying the filepath. \n",
    "    Each row is of the form patient_id, event_id, value.\n",
    "    The csv file should have a header \n",
    "    For example if you are using Pandas, you could write: \n",
    "        filtered_events.to_csv(deliverables_path + 'etl_filtered_events.csv', columns=['patient_id', 'event_id', 'value'], index=False)\n",
    "\n",
    "    Return filtered_events\n",
    "    '''\n",
    "\n",
    "    \n",
    "    filtered_events0 = pd.merge(events, indx_date, on = ['patient_id'])\n",
    "    filtered_events0['indx_date'] = pd.to_datetime(filtered_events0['indx_date'])\n",
    "    filtered_events0['timestamp'] = pd.to_datetime(filtered_events0['timestamp'])\n",
    "    filtered_events = filtered_events0.copy()\n",
    "    filtered_events = filtered_events[(filtered_events.timestamp <= filtered_events.indx_date) \n",
    "    & (filtered_events.timestamp >= filtered_events.indx_date-timedelta(days = 2000))]\n",
    "    filtered_events.to_csv(deliverables_path + 'etl_filtered_events.csv', \n",
    "                           columns=['patient_id', 'event_id', 'value'], index=False)\n",
    "    return filtered_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_events(filtered_events_df, mortality_df,feature_map_df, deliverables_path):\n",
    "    \n",
    "    '''\n",
    "    steps:\n",
    "    1. Replace event_id's with index available in event_feature_map.csv\n",
    "    2. Remove events with n/a values\n",
    "    3. Aggregate events using sum and count to calculate feature value\n",
    "    4. Normalize the values obtained above using min-max normalization(the min value will be 0 in all scenarios) \n",
    "    \n",
    "    IMPORTANT:\n",
    "    Save aggregated_events to a csv file in the deliverables folder named as etl_aggregated_events.csv. \n",
    "    Use the global variable deliverables_path while specifying the filepath. \n",
    "    Each row is of the form patient_id, event_id, value.\n",
    "    The csv file should have a header .\n",
    "    For example if you are using Pandas, you could write: \n",
    "        aggregated_events.to_csv(deliverables_path + 'etl_aggregated_events.csv', columns=['patient_id', 'feature_id', 'feature_value'], index=False)\n",
    "\n",
    "    Return aggregated_events\n",
    "    '''\n",
    "    aggregated_events0 = pd.merge(filtered_events_df, feature_map_df, on = 'event_id')    \n",
    "    aggregated_events0 = aggregated_events0[['patient_id','idx','value']]\n",
    "    aggregated_events0 = aggregated_events0[pd.notnull(aggregated_events0['value'])]  \n",
    "    MedicineAgg = aggregated_events0[aggregated_events0['idx'] < 2680]    \n",
    "    SumMedicineAgg = MedicineAgg.groupby(['patient_id','idx']).agg('sum').reset_index()\n",
    "    SumMedicineAggMax = SumMedicineAgg.groupby(['idx']).agg('max').reset_index()\n",
    "    SumMedicineAggMax = SumMedicineAggMax.rename(columns = {\"value\": \"max\"}) \n",
    "    SumMedicineAggMax = SumMedicineAggMax.drop(['patient_id'], axis=1)\n",
    "\n",
    "    SumMedicineAgg = pd.merge(SumMedicineAgg, SumMedicineAggMax, on = 'idx')\n",
    "    SumMedicineAgg[\"value2\"] = SumMedicineAgg[\"value\"]/SumMedicineAgg[\"max\"]\n",
    "    SumMedicineAgg = SumMedicineAgg.drop(['value', 'max'], axis=1)\n",
    "    SumMedicineAgg = SumMedicineAgg.rename(columns = {\"value2\": \"feature_value\", \"idx\": \"feature_id\"})\n",
    "    TestID = aggregated_events0[aggregated_events0['idx'] >= 2680]    \n",
    "    TestAgg = TestID.groupby(['patient_id','idx']).agg('count').reset_index() \n",
    "    TestAggMax = TestAgg.groupby(['idx']).agg('max').reset_index()   \n",
    "    TestAggMax = TestAggMax.rename(columns = {\"value\": \"max\"}) \n",
    "    TestAggMax = TestAggMax.drop(['patient_id'], axis=1)\n",
    "\n",
    "    TestAgg = pd.merge(TestAgg, TestAggMax, on = 'idx')\n",
    "    TestAgg[\"value2\"] = TestAgg[\"value\"]/TestAgg[\"max\"]\n",
    "    TestAgg = TestAgg.drop(['value', 'max'], axis=1)\n",
    "    TestAgg = TestAgg.rename(columns = {\"value2\": \"feature_value\", \"idx\": \"feature_id\"})\n",
    "    aggregated_events = pd.concat([SumMedicineAgg, TestAgg]).reset_index(drop = True)\n",
    "    aggregated_events.to_csv(deliverables_path + 'etl_aggregated_events.csv', columns=['patient_id', 'feature_id', 'feature_value'], index=False)\n",
    "    \n",
    "    return aggregated_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(events, mortality, feature_map):\n",
    "    \n",
    "    deliverables_path = 'output/'\n",
    "    #Calculate index date\n",
    "    indx_date = calculate_index_date(events, mortality, deliverables_path)\n",
    "    #Filter events in the observation window\n",
    "    filtered_events = filter_events(events, indx_date,  deliverables_path)    \n",
    "    #Aggregate the event values for each patient \n",
    "    aggregated_events = aggregate_events(filtered_events, mortality, feature_map, deliverables_path)\n",
    "    '''\n",
    "    Complete the code below by creating two dictionaries - \n",
    "    1. patient_features :  Key - patient_id and value is array of tuples(feature_id, feature_value)\n",
    "    2. mortality : Key - patient_id and value is mortality label\n",
    "    '''\n",
    "    patient_features = aggregated_events.groupby('patient_id')[['feature_id','feature_value']].apply(lambda x: [tuple(x) for x in x.values]).to_dict()\n",
    "    events_3 = events.copy()\n",
    "    events_3 = events_3.assign(deceased=events_3['patient_id'].isin(mortality['patient_id']).astype(int))\n",
    "    surviveData = events_3.copy()\n",
    "    surviveData = surviveData.loc[surviveData['deceased'] == 0]\n",
    "    deceasedData = events_3.copy()\n",
    "    deceasedData = deceasedData.loc[deceasedData['deceased'] == 1]\n",
    "    #join two talbes\n",
    "    ConcatedTable = pd.concat([surviveData, deceasedData]).reset_index(drop = True)\n",
    "    ConcatedTable = ConcatedTable.drop(['event_id', 'event_description', 'timestamp', 'value'], axis=1) \n",
    "    mortality = pd.Series(ConcatedTable.deceased.values, index = ConcatedTable.patient_id).to_dict()\n",
    "    return patient_features, mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_svmlight(patient_features, mortality, op_file, op_deliverable):\n",
    "    \n",
    "    '''\n",
    "    Create two files:\n",
    "    1. op_file - which saves the features in svmlight format. (See instructions in Q3d for detailed explanation)\n",
    "    2. op_deliverable - which saves the features in following format:\n",
    "       patient_id1 label feature_id:feature_value feature_id:feature_value feature_id:feature_value ...\n",
    "       patient_id2 label feature_id:feature_value feature_id:feature_value feature_id:feature_value ...  \n",
    "    \n",
    "    Note: Please make sure the features are ordered in ascending order, and patients are stored in ascending order as well.     \n",
    "    '''\n",
    "    deliverable1 = open(op_file, 'wb')\n",
    "    deliverable2 = open(op_deliverable, 'wb')\n",
    "    for patient_id in sorted(patient_features):\n",
    "        feature_pairs_string = \"\"\n",
    "        for feature in sorted(patient_features[patient_id]):\n",
    "            feature_pairs_string += \" \" + str(int(feature[0])) + \":\" + format(feature[1], '.6f')\n",
    "        svm_light_str = str(mortality[patient_id]) + feature_pairs_string + \" \\n\"\n",
    "        deliverable1.write(bytes((svm_light_str),'UTF-8')); #Use 'UTF-8'\n",
    "        deliverable2.write(bytes((str(int(patient_id)) + \" \" + svm_light_str),'UTF-8'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_path = 'data/train/'\n",
    "    events, mortality, feature_map = read_csv(train_path)\n",
    "    patient_features, mortality = create_features(events, mortality, feature_map)\n",
    "    save_svmlight(patient_features, mortality, 'output/features_svmlight.train', 'output/features.train')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output for model building\n",
    "features_svmlight.train,features.train,etl_index_dates.csv, etl_filtered_events.csv, etl_aggregated_events"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
