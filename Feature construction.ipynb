{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature construction\n",
    "import utils\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def read_csv(filepath):\n",
    "    \n",
    "    '''    \n",
    "    Read the events.csv, mortality_events.csv and event_feature_map.csv files into events, mortality and feature_map.\n",
    "    Return events, mortality and feature_map\n",
    "    '''\n",
    "    #Columns in events.csv - patient_id,event_id,event_description,timestamp,value\n",
    "    events = pd.read_csv(filepath + 'events.csv')\n",
    "    \n",
    "    #Columns in mortality_event.csv - patient_id,timestamp,label\n",
    "    mortality = pd.read_csv(filepath + 'mortality_events.csv')\n",
    "\n",
    "    #Columns in event_feature_map.csv - idx,event_id\n",
    "    feature_map = pd.read_csv(filepath + 'event_feature_map.csv')\n",
    "    return events, mortality, feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_index_date(events, mortality, deliverables_path):\n",
    "    \n",
    "    '''\n",
    "    1. Create list of patients alive ( mortality_events.csv only contains information about patients deceased)\n",
    "    2. Split events into two groups based on whether the patient is alive or deceased\n",
    "    3. Calculate index date for each patient\n",
    "    \n",
    "    IMPORTANT:\n",
    "    Save indx_date to a csv file in the deliverables folder named as etl_index_dates.csv. \n",
    "    Use the global variable deliverables_path while specifying the filepath. \n",
    "    Each row is of the form patient_id, indx_date.\n",
    "    The csv file should have a header \n",
    "    For example if you are using Pandas, you could write: \n",
    "        indx_date.to_csv(deliverables_path + 'etl_index_dates.csv', columns=['patient_id', 'indx_date'], index=False)\n",
    "\n",
    "    Return indx_date\n",
    "    '''\n",
    "    events_3 = events.copy()\n",
    "    events_3 = events_3.assign(deceased=events_3['patient_id'].isin(mortality['patient_id']).astype(int))\n",
    "    surviveData = events_3.copy()\n",
    "    surviveData = surviveData.loc[surviveData['deceased'] == 0]\n",
    "    surviveData = surviveData.drop(['deceased', 'event_id', 'event_description', 'value'], axis=1)\n",
    "    surviveData = surviveData.drop_duplicates()\n",
    "    surviveData['timestamp'] =pd.to_datetime(surviveData.timestamp)\n",
    "    surviveDataFinal = surviveData.loc[surviveData.groupby('patient_id').timestamp.idxmax()]\n",
    "    \n",
    "    deceasedData = mortality.copy()\n",
    "    deceasedData= deceasedData[['patient_id','timestamp']]\n",
    "    deceasedData['timestamp']= pd.to_datetime(deceasedData['timestamp'])\n",
    "    deceasedData['timestamp']= deceasedData['timestamp']-timedelta(days = 30)\n",
    "    \n",
    "    indx_date = pd.concat([surviveDataFinal,deceasedData]).reset_index(drop=True)\n",
    "    indx_date = indx_date.rename(columns={'timestamp': 'indx_date'})\n",
    "    indx_date.to_csv(deliverables_path + 'etl_index_dates.csv', columns=['patient_id', 'indx_date'], index=False)\n",
    "    \n",
    "    return indx_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_events(events, indx_date, deliverables_path):\n",
    "    \n",
    "    '''\n",
    "    1. Join indx_date with events on patient_id\n",
    "    2. Filter events occuring in the observation window(IndexDate-2000 to IndexDate)\n",
    "    Save filtered_events to a csv file in the deliverables folder named as etl_filtered_events.csv. \n",
    "    Use the global variable deliverables_path while specifying the filepath. \n",
    "    Each row is of the form patient_id, event_id, value.\n",
    "    The csv file should have a header \n",
    "    For example if you are using Pandas, you could write: \n",
    "        filtered_events.to_csv(deliverables_path + 'etl_filtered_events.csv', columns=['patient_id', 'event_id', 'value'], index=False)\n",
    "\n",
    "    Return filtered_events\n",
    "    '''\n",
    "\n",
    "    \n",
    "    filtered_events0 = pd.merge(events, indx_date, on = ['patient_id'])\n",
    "    filtered_events0['indx_date'] = pd.to_datetime(filtered_events0['indx_date'])\n",
    "    filtered_events0['timestamp'] = pd.to_datetime(filtered_events0['timestamp'])\n",
    "    filtered_events = filtered_events0.copy()\n",
    "    filtered_events = filtered_events[(filtered_events.timestamp <= filtered_events.indx_date) \n",
    "    & (filtered_events.timestamp >= filtered_events.indx_date-timedelta(days = 2000))]\n",
    "    filtered_events.to_csv(deliverables_path + 'etl_filtered_events.csv', \n",
    "                           columns=['patient_id', 'event_id', 'value'], index=False)\n",
    "    return filtered_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_events(filtered_events_df, mortality_df,feature_map_df, deliverables_path):\n",
    "    \n",
    "    '''\n",
    "    steps:\n",
    "    1. Replace event_id's with index available in event_feature_map.csv\n",
    "    2. Remove events with n/a values\n",
    "    3. Aggregate events using sum and count to calculate feature value\n",
    "    4. Normalize the values obtained above using min-max normalization(the min value will be 0 in all scenarios) \n",
    "    \n",
    "    IMPORTANT:\n",
    "    Save aggregated_events to a csv file in the deliverables folder named as etl_aggregated_events.csv. \n",
    "    Use the global variable deliverables_path while specifying the filepath. \n",
    "    Each row is of the form patient_id, event_id, value.\n",
    "    The csv file should have a header .\n",
    "    For example if you are using Pandas, you could write: \n",
    "        aggregated_events.to_csv(deliverables_path + 'etl_aggregated_events.csv', columns=['patient_id', 'feature_id', 'feature_value'], index=False)\n",
    "\n",
    "    Return aggregated_events\n",
    "    '''\n",
    "    aggregated_events0 = pd.merge(filtered_events_df, feature_map_df, on = 'event_id')    \n",
    "    aggregated_events0 = aggregated_events0[['patient_id','idx','value']]\n",
    "    aggregated_events0 = aggregated_events0[pd.notnull(aggregated_events0['value'])]  \n",
    "    MedicineAgg = aggregated_events0[aggregated_events0['idx'] < 2680]    \n",
    "    SumMedicineAgg = MedicineAgg.groupby(['patient_id','idx']).agg('sum').reset_index()\n",
    "    SumMedicineAggMax = SumMedicineAgg.groupby(['idx']).agg('max').reset_index()\n",
    "    SumMedicineAggMax = SumMedicineAggMax.rename(columns = {\"value\": \"max\"}) \n",
    "    SumMedicineAggMax = SumMedicineAggMax.drop(['patient_id'], axis=1)\n",
    "\n",
    "    SumMedicineAgg = pd.merge(SumMedicineAgg, SumMedicineAggMax, on = 'idx')\n",
    "    SumMedicineAgg[\"value2\"] = SumMedicineAgg[\"value\"]/SumMedicineAgg[\"max\"]\n",
    "    SumMedicineAgg = SumMedicineAgg.drop(['value', 'max'], axis=1)\n",
    "    SumMedicineAgg = SumMedicineAgg.rename(columns = {\"value2\": \"feature_value\", \"idx\": \"feature_id\"})\n",
    "    TestID = aggregated_events0[aggregated_events0['idx'] >= 2680]    \n",
    "    TestAgg = TestID.groupby(['patient_id','idx']).agg('count').reset_index() \n",
    "    TestAggMax = TestAgg.groupby(['idx']).agg('max').reset_index()   \n",
    "    TestAggMax = TestAggMax.rename(columns = {\"value\": \"max\"}) \n",
    "    TestAggMax = TestAggMax.drop(['patient_id'], axis=1)\n",
    "\n",
    "    TestAgg = pd.merge(TestAgg, TestAggMax, on = 'idx')\n",
    "    TestAgg[\"value2\"] = TestAgg[\"value\"]/TestAgg[\"max\"]\n",
    "    TestAgg = TestAgg.drop(['value', 'max'], axis=1)\n",
    "    TestAgg = TestAgg.rename(columns = {\"value2\": \"feature_value\", \"idx\": \"feature_id\"})\n",
    "    aggregated_events = pd.concat([SumMedicineAgg, TestAgg]).reset_index(drop = True)\n",
    "    aggregated_events.to_csv(deliverables_path + 'etl_aggregated_events.csv', columns=['patient_id', 'feature_id', 'feature_value'], index=False)\n",
    "    \n",
    "    return aggregated_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(events, mortality, feature_map):\n",
    "    \n",
    "    deliverables_path = 'output/'\n",
    "    #Calculate index date\n",
    "    indx_date = calculate_index_date(events, mortality, deliverables_path)\n",
    "    #Filter events in the observation window\n",
    "    filtered_events = filter_events(events, indx_date,  deliverables_path)    \n",
    "    #Aggregate the event values for each patient \n",
    "    aggregated_events = aggregate_events(filtered_events, mortality, feature_map, deliverables_path)\n",
    "    '''\n",
    "    Complete the code below by creating two dictionaries - \n",
    "    1. patient_features :  Key - patient_id and value is array of tuples(feature_id, feature_value)\n",
    "    2. mortality : Key - patient_id and value is mortality label\n",
    "    '''\n",
    "    patient_features = aggregated_events.groupby('patient_id')[['feature_id','feature_value']].apply(lambda x: [tuple(x) for x in x.values]).to_dict()\n",
    "    events_3 = events.copy()\n",
    "    events_3 = events_3.assign(deceased=events_3['patient_id'].isin(mortality['patient_id']).astype(int))\n",
    "    surviveData = events_3.copy()\n",
    "    surviveData = surviveData.loc[surviveData['deceased'] == 0]\n",
    "    deceasedData = events_3.copy()\n",
    "    deceasedData = deceasedData.loc[deceasedData['deceased'] == 1]\n",
    "    #join two talbes\n",
    "    ConcatedTable = pd.concat([surviveData, deceasedData]).reset_index(drop = True)\n",
    "    ConcatedTable = ConcatedTable.drop(['event_id', 'event_description', 'timestamp', 'value'], axis=1) \n",
    "    mortality = pd.Series(ConcatedTable.deceased.values, index = ConcatedTable.patient_id).to_dict()\n",
    "    return patient_features, mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_svmlight(patient_features, mortality, op_file, op_deliverable):\n",
    "    \n",
    "    '''\n",
    "    Create two files:\n",
    "    1. op_file - which saves the features in svmlight format. (See instructions in Q3d for detailed explanation)\n",
    "    2. op_deliverable - which saves the features in following format:\n",
    "       patient_id1 label feature_id:feature_value feature_id:feature_value feature_id:feature_value ...\n",
    "       patient_id2 label feature_id:feature_value feature_id:feature_value feature_id:feature_value ...  \n",
    "    \n",
    "    Note: Please make sure the features are ordered in ascending order, and patients are stored in ascending order as well.     \n",
    "    '''\n",
    "    deliverable1 = open(op_file, 'wb')\n",
    "    deliverable2 = open(op_deliverable, 'wb')\n",
    "    for patient_id in sorted(patient_features):\n",
    "        feature_pairs_string = \"\"\n",
    "        for feature in sorted(patient_features[patient_id]):\n",
    "            feature_pairs_string += \" \" + str(int(feature[0])) + \":\" + format(feature[1], '.6f')\n",
    "        svm_light_str = str(mortality[patient_id]) + feature_pairs_string + \" \\n\"\n",
    "        deliverable1.write(bytes((svm_light_str),'UTF-8')); #Use 'UTF-8'\n",
    "        deliverable2.write(bytes((str(int(patient_id)) + \" \" + svm_light_str),'UTF-8'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_path = 'data/train/'\n",
    "    events, mortality, feature_map = read_csv(train_path)\n",
    "    patient_features, mortality = create_features(events, mortality, feature_map)\n",
    "    save_svmlight(patient_features, mortality, 'output/features_svmlight.train', 'output/features.train')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output:features_svmlight.train,features.train,etl_index_dates.csv, etl_filtered_events.csv, etl_aggregated_events"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
